{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How to Get Zillow Property Listings with Python 2023</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.scrapeak.com/zillow-scraper/?ref=ariel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/zapimoveis-scraper/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly.express'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ef587247adfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly.express'"
     ]
    }
   ],
   "source": [
    "#!pip install plotly.express\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "#settings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-81fc88b5495a>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-81fc88b5495a>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    return = requests.request(\"GET\", url, params=querystring)\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_listings(api_key, listing_url):\n",
    "    url = \"https://app.scrapeak.com/v1/scrapers/zillow/listing\"\n",
    "    \n",
    "    querystring = {\n",
    "        \"api_key\": api_key,\n",
    "        \"url\":linting_url\n",
    "    }\n",
    "    \n",
    "    return = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "def get_property_detail(api_key, zpid):\n",
    "    url = \"https://app.scrapeak.com/v1/scrapers/zillow/property\"\n",
    "    \n",
    "    querystring = {\n",
    "        \"api_key\": api_key,\n",
    "        \"url\":zpid\n",
    "    }\n",
    "    \n",
    "    return requests.request(\"GET\", URL, params=querystring)\n",
    "\n",
    "def get_zpid(spi_key, street, city, state, zip_code=None):\n",
    "    url = \"https://app.scrapeak.com/v1/scrapers/zillow/zpidByAddress\"\n",
    "    \n",
    "    querystring = {\n",
    "        \"api_key\": api_key,\n",
    "        \"street\": street,\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"zip_code\": zip_code\n",
    "    }\n",
    "    \n",
    "    return requests.request(\"GET\", url, params=querystring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Locals & Constants</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Web Scraping Stocks</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função Para Scraping de Dados Históricos da NASDAQ\n",
    "def scrape_nasdaq(ticker_list):\n",
    "    tckr_data = {}\n",
    "    count = 1\n",
    "    \n",
    "    for ticker in ticker_list:\n",
    "        print(\"Scraping count: \" + str(count))\n",
    "        \n",
    "        url = 'https://old.nasdaq.com/symbol/' + ticker + '/historical'\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Coleta de 10 anos de dados\n",
    "        data_range = driver.find_elements_by_name('ddlTimeFrame')\n",
    "        \n",
    "        if len(data_range) > 0:\n",
    "            for option in data_range[0].find_elements_by_tag_name('option'):\n",
    "                if option.text == '10 Years':\n",
    "                    option.click()\n",
    "                    break\n",
    "            time.sleep(5)\n",
    "            page_source = driver.page_source\n",
    "    \n",
    "            # Gerando objeto soup para o parse de dados da URL\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "            # Definindo a regra de pesquisa na tabela de dados históricos\n",
    "            tags = soup.find_all('div', id = \"historicalContainer\")\n",
    "    \n",
    "            # Iniciando a busca\n",
    "            temp_data = []\n",
    "    \n",
    "            for tag in tags:\n",
    "                rows = tag.findAll('tr')\n",
    "                for tr in rows:\n",
    "                    cols = tr.findAll('td')\n",
    "                    val = [tr.text for tr in cols]\n",
    "                    temp_data.append(val)\n",
    "            \n",
    "            # Limpando os dados\n",
    "            for i in range(len(temp_data)):\n",
    "                to_process = temp_data[i]\n",
    "    \n",
    "                for i in range(len(to_process)):\n",
    "                    temp = to_process[i]\n",
    "                    temp = temp.strip()\n",
    "                    to_process[i] = temp\n",
    "         \n",
    "            # Obtendo os dados\n",
    "            temp_data = temp_data[2:]\n",
    "            \n",
    "            if tckr_data.get(ticker) == None:\n",
    "                tckr_data[ticker] = temp_data\n",
    "        \n",
    "            # Pausa na coleta para evitar bloqueio no site da Nasdaq\n",
    "            print(\"Random Sleep\")\n",
    "            sleep(randint(2, 4))\n",
    "            count += 1\n",
    "        \n",
    "        else:\n",
    "            print(\"Não Encontrado\" + ticker)\n",
    "        \n",
    "    # Preparando as listas para o dataframe\n",
    "    date = []\n",
    "    Open = []\n",
    "    high = []\n",
    "    low = []\n",
    "    close = []\n",
    "    volume = []\n",
    "    ticker = []\n",
    "\n",
    "    # Coletando e classificando os dados\n",
    "    for key, value in tckr_data.items():\n",
    "        for data in value:\n",
    "            date.append(data[0].replace('/','-'))\n",
    "            Open.append(data[1])\n",
    "            high.append(data[2])\n",
    "            low.append(data[3])\n",
    "            close.append(data[4])\n",
    "            volume.append(data[5].replace(',',''))\n",
    "            ticker.append(key)  \n",
    "            \n",
    "    # Dicionário final com os dados\n",
    "    final_dict = {'date' : date, 'Open':Open, 'High':high, 'Low':low, 'Close':close, 'Volume':volume, 'ticker':ticker}\n",
    "    \n",
    "    # Gerando o dataframe a partir do dicionário\n",
    "    df = pd.DataFrame(final_dict)\n",
    "    cols = ['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'ticker']\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função Para Obter Dados de Ações do Yahoo Finance\n",
    "def retorna_acoes(ticker_list, start, end):\n",
    "    list_of_df = []\n",
    "    not_found = []\n",
    "    \n",
    "    for ticker in ticker_list:\n",
    "        try:\n",
    "            df = pdr.get_data_yahoo(ticker, start, end)\n",
    "            df = df.reset_index()\n",
    "            df.rename(columns = {'Date':'date'})\n",
    "            df['ticker'] = [ticker] * len(df)\n",
    "            list_of_df.append(df)\n",
    "        except ValueError:\n",
    "            print(\"Não Encontrado: \" + ticker)\n",
    "            not_found.append(ticker)\n",
    "    \n",
    "    return list_of_df, not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Para Obter as Cotações Históricas Usando as Duas Funções Acima\n",
    "def get_stocks_data(ticker_set, start, end):\n",
    "    \n",
    "    # Retorna ações que que serão coletadas\n",
    "    stocks_1, not_found = retorna_acoes(ticker_set, start, end)\n",
    "    \n",
    "    if len(not_found) > 0:\n",
    "    \n",
    "        # Dados das ações da Nasdaq\n",
    "        stocks = scrape_nasdaq(not_found)\n",
    "    \n",
    "        # Merging em um dataframe\n",
    "        stocks_1 = pd.concat(stocks_1)\n",
    "\n",
    "        stocks_1 = stocks_1.rename(columns = {'Date' : 'date'})\n",
    "        stocks_1 = stocks_1[['date', 'Open', 'High', 'Low', 'Close', 'Volume', 'ticker']]\n",
    "        stocks_1['date'] = stocks_1['date'].dt.date\n",
    "\n",
    "        stocks = stocks.rename(columns = {'Ticker' : 'ticker'})\n",
    "        stocks[['Open', 'High', 'Low', 'Close', 'Volume']] = stocks[['Open', 'High', 'Low', 'Close', 'Volume']].astype(float)\n",
    "    \n",
    "        stocks_final = pd.concat([stocks_1, stocks])\n",
    "        return stocks_final\n",
    "    \n",
    "    else:\n",
    "        return pd.concat(stocks_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
